import pandas as pd
import config
import file_manager
import crawl_manager
import ai_manager
import time

def main():
    print("ğŸš€ B2B é”€å”® Agent v10.0 (ä¸‰æ€ç»ˆæç‰ˆ) å¯åŠ¨...")
    print("------------------------------------------------")
    
    # 1. åŠ è½½èµ„æº
    sop_text = file_manager.load_sop()
    if not sop_text: return

    df = file_manager.init_excel()
    if df is None: return

    # ================= æ ¸å¿ƒé…ç½® =================
    # æ¯è·‘ 10 å®¶å…¬å¸ï¼Œé‡å¯ä¸€æ¬¡æµè§ˆå™¨ï¼ˆæ¢IP + æ¸…å†…å­˜ï¼‰
    # å¦‚æœæ‚¨è§‰å¾—ä»£ç†å¤Ÿç¨³ï¼Œå¯ä»¥æŠŠè¿™ä¸ªæ•°å­—è°ƒå¤§ï¼Œæ¯”å¦‚ 20 æˆ– 50
    BATCH_SIZE = 10  
    # ==========================================

    crawler = None
    current_batch_count = 0
    total = len(df)
    
    try:
        for index, row in df.iterrows():
            # æ–­ç‚¹ç»­ä¼ ï¼šå¦‚æœ Reason ä¸ä¸ºç©ºï¼Œè¯´æ˜è·‘è¿‡äº†ï¼Œè·³è¿‡
            if pd.notna(row.get('Reason')) and str(row.get('Reason')).strip() != "": 
                continue
            
            company = str(row['COMPANY_NAME']).strip()
            if not company or company == "nan": continue

            # ========================================================
            # æµè§ˆå™¨ç”Ÿå‘½å‘¨æœŸç®¡ç†ï¼šå¯åŠ¨ æˆ– é‡å¯
            # ========================================================
            if crawler is None or current_batch_count >= BATCH_SIZE:
                if crawler: 
                    print(f"\nğŸ”„ [ç»´æŠ¤] å·²è·‘å®Œ {BATCH_SIZE} å®¶ï¼Œé‡å¯æµè§ˆå™¨ä»¥åˆ‡æ¢ IP/é‡Šæ”¾å†…å­˜...")
                    try: crawler.close()
                    except: pass
                    time.sleep(2) # å†·å´ä¸€ä¸‹
                else:
                    print(f"\nğŸ”Œ [å¯åŠ¨] æ­£åœ¨å»ºç«‹è¿æ¥...")

                crawler = crawl_manager.Crawler()
                current_batch_count = 0 # è®¡æ•°å™¨å½’é›¶
            
            # ========================================================

            print(f"[{index+1}/{total}] {company}", end=" ", flush=True)

            try:
                # 1. çˆ¬å–
                text_data = crawler.search_and_crawl(company)
                print(" -> ğŸ§ ", end="")
                
                # 2. AI åˆ†æ
                result = ai_manager.get_analysis(text_data, sop_text)
                
                # 3. è§£æçŠ¶æ€ (ä¸‰æ€é€»è¾‘)
                # ä¼˜å…ˆè·å– 'status' å­—æ®µï¼Œå¦‚æœæ²¡æœ‰åˆ™ç»™é»˜è®¤å€¼ 'Review'
                status = result.get('status', 'Review') 
                
                # å…¼å®¹æ€§é˜²å¾¡ï¼šä¸‡ä¸€ AI å¶å°”æŠ½é£å›äº†å¸ƒå°”å€¼ï¼Œåšä¸ªæ˜ å°„
                if status is True or str(status).lower() == 'true': status = "Target"
                if status is False or str(status).lower() == 'false': status = "Pass"
                
                # 4. è®°å½•ç»“æœ
                df.at[index, 'Is_Target'] = status
                df.at[index, 'Target_Products'] = str(result.get('target_products', []))
                df.at[index, 'Reason'] = result.get('reason', 'Unknown')
                
                # 5. ç»ˆç«¯å¯è§†åŒ–åé¦ˆ (æ ¹æ®çŠ¶æ€æ˜¾ç¤ºä¸åŒå›¾æ ‡)
                if status == "Target":
                    icon = "âœ…" 
                elif status == "Pass":
                    icon = "â¬œ"
                else:
                    icon = "ğŸ¤”" # Review (å¾…æ ¸å®)
                
                print(f" {icon} [{status}]")

                # 6. å®æ—¶ä¿å­˜
                file_manager.save_excel(df)
                
                current_batch_count += 1

            except Exception as e:
                print(f"\n   âŒ Error: {e}")
                # è®°å½•é”™è¯¯ï¼Œé˜²æ­¢ä¸‹æ¬¡å¡åœ¨è¿™é‡Œæ­»å¾ªç¯ï¼ˆæˆ–è€…æ‚¨å¯ä»¥é€‰æ‹©ä¸è®°å½•ï¼Œè®©å®ƒä¸‹æ¬¡é‡è¯•ï¼‰
                df.at[index, 'Reason'] = f"Error: {e}"
                file_manager.save_excel(df)
                
                # å¦‚æœæŠ¥é”™äº†ï¼Œä¸ºäº†å®‰å…¨èµ·è§ï¼Œå¼ºåˆ¶ä¸‹ä¸€æ¬¡å¾ªç¯é‡å¯æ¢IP
                current_batch_count = BATCH_SIZE 

    except KeyboardInterrupt:
        print("\nğŸ‘‹ ç”¨æˆ·æ‰‹åŠ¨åœæ­¢ä»»åŠ¡")
    
    finally:
        print("------------------------------------------------")
        print("ğŸ‰ ä»»åŠ¡å…¨éƒ¨å®Œæˆï¼")
        if crawler: 
            try: crawler.close()
            except: pass

if __name__ == "__main__":
    main()