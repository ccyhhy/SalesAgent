from DrissionPage import ChromiumPage, ChromiumOptions
import time
import random
import urllib.parse
import config
import os
import shutil

class Crawler:
    def __init__(self):
        co = ChromiumOptions()
        co.no_imgs(True)
        co.mute(True)
        

        
        # 1. å¼ºåˆ¶æµè§ˆå™¨èµ°ä»£ç† (åŸç”Ÿå‚æ•°ï¼Œæœ€ç¨³ï¼Œæµè§ˆå™¨æ— æ³•å¿½ç•¥)
        co.set_argument(f'--proxy-server={self.PROXY_HOST}:{self.PROXY_PORT}')
        
        # 2. åŠ è½½â€œåªè´Ÿè´£å¡«å¯†ç â€çš„æ’ä»¶ (è§£å†³å¼¹çª—é—®é¢˜)
        self.plugin_path = self._create_auth_plugin(self.PROXY_USER, self.PROXY_PASS)
        co.add_extension(self.plugin_path)
        
        # 3. æµè§ˆå™¨è®°å¿†
        user_data_path = os.path.join(os.getcwd(), 'browser_data')
        co.set_user_data_path(user_data_path)
        
        try:
            self.page = ChromiumPage(co)
            self.page.set.load_mode.eager()
            print(f"ğŸŒ æµè§ˆå™¨å·²å¯åŠ¨ (åŸç”Ÿä»£ç†+æ’ä»¶è®¤è¯)")
            
            # ã€å¼ºåˆ¶è‡ªæ£€ã€‘å¯åŠ¨æ—¶ç«‹åˆ»æŸ¥ä¸€æ¬¡ IPï¼Œè®©ä½ çœ¼è§ä¸ºå®
            print("   ğŸ•µï¸â€â™‚ï¸ æ­£åœ¨éªŒè¯ä»£ç†è¿æ¥...", end="")
            self.page.get('http://httpbin.org/ip', timeout=10)
            # è·å–é¡µé¢æ˜¾ç¤ºçš„ IP
            ip_info = self.page.ele('tag:body').text
            print(f" -> {ip_info}")
            
        except Exception as e:
            print(f"\nâŒ å¯åŠ¨è‡ªæ£€å¤±è´¥: {e} (å¯èƒ½æ˜¯ä»£ç†è¶…æ—¶æˆ–é…ç½®é”™è¯¯)")

    def _create_auth_plugin(self, user, password):
        """
        ç”Ÿæˆä¸€ä¸ªã€çº¯ç²¹ã€‘çš„è®¤è¯æ’ä»¶
        å®ƒä¸å†è®¾ç½®ä»£ç†åœ°å€(å› ä¸ºä¸Šé¢ç”¨å‚æ•°è®¾äº†)ï¼Œåªè´Ÿè´£å¡«å¯†ç ã€‚
        """
        plugin_path = os.path.join(os.getcwd(), 'proxy_auth_plugin')
        
        if os.path.exists(plugin_path):
            shutil.rmtree(plugin_path)
        os.makedirs(plugin_path)

        manifest_json = """
        {
            "version": "1.0.0",
            "manifest_version": 3,
            "name": "Chrome Proxy Auth Helper",
            "permissions": ["proxy", "webRequest", "webRequestBlocking"],
            "host_permissions": ["<all_urls>"],
            "background": {"service_worker": "background.js"}
        }
        """

        # è¿™ä¸ªè„šæœ¬åªåšä¸€ä»¶äº‹ï¼šå¬åˆ°è¦å¯†ç ï¼Œå°±å¡«è¿›å»
        background_js = f"""
        chrome.webRequest.onAuthRequired.addListener(
            function(details) {{
                return {{
                    authCredentials: {{
                        username: "{user}",
                        password: "{password}"
                    }}
                }};
            }},
            {{urls: ["<all_urls>"]}},
            ['blocking']
        );
        """

        with open(os.path.join(plugin_path, "manifest.json"), "w", encoding='utf-8') as f:
            f.write(manifest_json)
        with open(os.path.join(plugin_path, "background.js"), "w", encoding='utf-8') as f:
            f.write(background_js)
        
        return plugin_path

    def search_and_crawl(self, company_name):
        """å¸¦é‡è¯•æœºåˆ¶çš„ä»»åŠ¡æ‰§è¡Œ"""
        max_retries = 3
        
        for attempt in range(max_retries):
            try:
                return self._execute_task(company_name)
            
            except Exception as e:
                print(f" (âš ï¸ å¼‚å¸¸: {str(e)[:30]}...)")
                
                if attempt < max_retries - 1:
                    print("   ğŸ”„ å¼ºåˆ¶é‡å¯æµè§ˆå™¨æ¢ IP...")
                    self._start_browser() 
                    time.sleep(2)
                else:
                    return f"ã€å¤±è´¥ã€‘å¤šæ¬¡é‡è¯•æ— æ•ˆ"

    def _start_browser(self):
        """é‡å¯æµè§ˆå™¨é€»è¾‘"""
        if self.page:
            try: self.page.quit()
            except: pass
        
        # é‡å¯æ—¶å¿…é¡»é‡æ–°èµ°ä¸€éé…ç½®æµç¨‹
        self.__init__()

    def _execute_task(self, company_name):
        if not self.page or not self.page.process_id:
            raise Exception("æµè§ˆå™¨å·²æ–­å¼€")

        page = self.page
        abstract = ""
        website_text = ""
        
        # --- Step 1: æœç´¢ ---
        query = urllib.parse.quote(f"{company_name} å®˜ç½‘")
        page.get(f"{config.SEARCH_ENGINE_URL}{query}", retry=1) 
        
        if "å®‰å…¨éªŒè¯" in page.title or page.ele('text:ç½‘ç»œä¸ç»™åŠ›'):
            raise Exception("è§¦å‘ç™¾åº¦éªŒè¯ç ") 

        # æŠ“æ‘˜è¦
        try:
            res = page.eles('css:#content_left .result', timeout=2)
            for r in res[:3]: abstract += r.text + "\n"
        except: pass

        # --- Step 2: è¿›ç«™ ---
        target_link = None
        try:
            res_list = page.eles('css:#content_left .result', timeout=2)
            for res in res_list[:5]:
                title = res.ele('tag:h3').text
                if any(x in title for x in ['æ‹›è˜', 'çˆ±ä¼æŸ¥', 'å¤©çœ¼æŸ¥', 'ä¼æŸ¥æŸ¥', '58åŒåŸ', 'å°çº¢ä¹¦', 'çŸ¥ä¹', 'è´´å§', 'ç™¾ç§‘']):
                    continue
                target_link = res.ele('tag:a')
                break
            
            if target_link:
                print("-> ğŸš€", end="")
                target_link.click()
                page.wait.new_tab()
                new_tab = page.latest_tab
                try:
                    new_tab.wait.ele('tag:body', timeout=10)
                    new_tab.scroll.to_bottom()
                    time.sleep(1)
                    website_text = new_tab.ele('tag:body').text
                    website_text = website_text.replace('\n', ' ')
                except:
                    website_text = "åŠ è½½è¶…æ—¶"
                new_tab.close()
            else:
                print("-> âš ï¸", end="")
                website_text = "æ— å®˜ç½‘"
        except:
            if len(page.tabs) > 1: page.latest_tab.close()

        return f"ã€ç™¾åº¦æ‘˜è¦ã€‘\n{abstract}\n\nã€å®˜ç½‘ã€‘\n{website_text}"

    def close(self):
        try: self.page.quit()
        except: pass
